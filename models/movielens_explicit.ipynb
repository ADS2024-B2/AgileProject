{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit feedback movie recommendations\n",
    "In this example, we'll build a quick explicit feedback recommender system: that is, a model that takes into account explicit feedback signals (like ratings) to recommend new content.\n",
    "\n",
    "We'll use an approach first made popular by the [Netflix prize](http://www.netflixprize.com/) contest: [matrix factorization](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf). \n",
    "\n",
    "The basic idea is very simple:\n",
    "\n",
    "1. Start with user-item-rating triplets, conveying the information that user _i_ gave some item _j_ rating _r_.\n",
    "2. Represent both users and items as high-dimensional vectors of numbers. For example, a user could be represented by `[0.3, -1.2, 0.5]` and an item by `[1.0, -0.3, -0.6]`.\n",
    "3. The representations should be chosen so that, when we multiplied together (via [dot products](https://en.wikipedia.org/wiki/Dot_product)), we can recover the original ratings.\n",
    "4. The utility of the model then is derived from the fact that if we multiply the user vector of a user with the item vector of some item they _have not_ rated, we hope to obtain a predicition for the rating they would have given to it had they seen it.\n",
    "\n",
    "<img src=\"static/matrix_factorization.png\" alt=\"Matrix factorization\" style=\"width: 600px;\"/>\n",
    "\n",
    "Spotlight fits models such as these using [stochastic gradient descent](http://cs231n.github.io/optimization-1/). The procedure goes roughly as follows:\n",
    "\n",
    "1. Start with representing users and items by randomly chosen vectors. Because they are random, they are not going to give useful recommendations, but we are going to improve them as we fit the model.\n",
    "2. Go through the (user, item, rating) triplets in the dataset. For every triplet, compute the rating that the model predicts by multiplying the user and item vectors together, and compare the result with the actual rating: the closer they are, the better the model.\n",
    "3. If the predicted rating is too low, adjust the user and item vectors (by a small amount) to increase the prediction.\n",
    "4. If the predicted rating is too high, adjust the vectors to decrease it.\n",
    "5. Continue iterating over the training triplets until the model's accuracy stabilizes.\n",
    "\n",
    "## The data\n",
    "\n",
    "\n",
    "\n",
    "We start with importing a famous dataset, the [Movielens 100k dataset](https://grouplens.org/datasets/movielens/100k/). It contains 100,000 ratings (between 1 and 5) given to 1683 movies by 944 users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spotlight.datasets.movielens import get_movielens_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (944 users x 1683 items x 100000 interactions)>\n"
     ]
    }
   ],
   "source": [
    "dataset = get_movielens_dataset(variant='100K')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dataset` object is an instance of an `Interactions` [class](https://maciejkula.github.io/spotlight/interactions.html#spotlight.interactions.Interactions), a fairly light-weight wrapper that Spotlight users to hold the arrays that contain information about an interactions dataset (such as user and item ids, ratings, and timestamps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We can feed our dataset to the [`ExplicitFactorizationModel`](https://maciejkula.github.io/spotlight/factorization/explicit.html#spotlight.factorization.explicit.ExplicitFactorizationModel) class - and sklearn-like object that allows us to train and evaluate the explicit factorization models.\n",
    "\n",
    "Internally, the model uses the [`BilinearNet`](https://maciejkula.github.io/spotlight/factorization/representations.html#spotlight.factorization.representations.BilinearNet) class to represents users and items. It's composed of a 4 [embedding layers](http://pytorch.org/docs/master/nn.html?highlight=embedding#torch.nn.Embedding):\n",
    "\n",
    "- a `(num_users x latent_dim)` embedding layer to represent users,\n",
    "- a `(num_items x latent_dim)` embedding layer to represent items,\n",
    "- a `(num_users x 1)` embedding layer to represent user biases, and\n",
    "- a `(num_items x 1)` embedding layer to represent item biases.\n",
    "\n",
    "Together, these give us the predictions. Their accuracy is evaluated using one of the Spotlight [losses](https://maciejkula.github.io/spotlight/losses.html). In this case, we'll use the [regression loss](https://maciejkula.github.io/spotlight/losses.html#spotlight.losses.regression_loss), which is simply the squared difference between the true and the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "\n",
    "model = ExplicitFactorizationModel(loss='regression',\n",
    "                                   embedding_dim=128,  # latent dimensionality\n",
    "                                   n_iter=10,  # number of epochs of training\n",
    "                                   batch_size=1024,  # minibatch size\n",
    "                                   l2=1e-9,  # strength of L2 regularization\n",
    "                                   learning_rate=1e-3\n",
    "                                #    ,use_cuda=torch.cuda.is_available()\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit and evaluate the model, we need to split it into a train and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into \n",
      " <Interactions dataset (944 users x 1683 items x 80000 interactions)> and \n",
      " <Interactions dataset (944 users x 1683 items x 20000 interactions)>.\n"
     ]
    }
   ],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "\n",
    "train, test = random_train_test_split(dataset, random_state=np.random.RandomState(42))\n",
    "\n",
    "print('Split into \\n {} and \\n {}.'.format(train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More or less the same users are in both train and test groups, but giving ratings to different movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data ready, we can go ahead and fit the model. This should take less than a minute on the CPU, and we should see the loss decreasing as the model is learning better and better representations for the user and items in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 13.088664392881755\n",
      "Epoch 1: loss 7.181213946282109\n",
      "Epoch 2: loss 1.7354737339140494\n",
      "Epoch 3: loss 1.0646275108373617\n",
      "Epoch 4: loss 0.9384114621560785\n",
      "Epoch 5: loss 0.894170845611186\n",
      "Epoch 6: loss 0.867523076413553\n",
      "Epoch 7: loss 0.8508917553515374\n",
      "Epoch 8: loss 0.8364696442326413\n",
      "Epoch 9: loss 0.8273540161832978\n"
     ]
    }
   ],
   "source": [
    "model.fit(train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is estimated, how good are its predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE 0.893, test RMSE 0.938\n"
     ]
    }
   ],
   "source": [
    "from spotlight.evaluation import rmse_score\n",
    "\n",
    "train_rmse = rmse_score(model, train)\n",
    "test_rmse = rmse_score(model, test)\n",
    "\n",
    "print('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this model as a pickle object and load it already trained from the streamlit script to make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'spotlight_explicit_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `predict` takes as input the id of a user and returns the predicted scores for all items in item_ids (movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = test.user_ids[2]\n",
    "rating_prediction = model.predict(new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of the top 50 highest values\n",
    "top_50_indices = np.argsort(rating_prediction)[-50:][::-1]\n",
    "random_5_indices = np.random.choice(top_50_indices, 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read extended dataset to translate movie id to movie title\n",
    "full_data = pd.read_csv('datasets/merged_movielens.txt', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information on top recommended movies\n",
    "recs = pd.DataFrame(columns=['movie_title', 'IMDb_URL', 'genres_name'])\n",
    "\n",
    "for movie_recommendation in random_5_indices:\n",
    "    # Get the first match row for the current recommendation\n",
    "    row = full_data[full_data['movie_id'] == movie_recommendation].iloc[0]\n",
    "    \n",
    "    # Add the relevant information from `row` to `recs`\n",
    "    recs.loc[len(recs)] = [\n",
    "        row['movie_title'], \n",
    "        row['IMDb_URL'], \n",
    "        row['genres_name']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>IMDb_URL</th>\n",
       "      <th>genres_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Star Wars (1977)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Star%20Wars%2...</td>\n",
       "      <td>['Action', 'Adventure', 'Romance', 'Sci-Fi', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>African Queen, The (1951)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?African%20Que...</td>\n",
       "      <td>['Action', 'Adventure', 'Romance', 'War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Raise the Red Lantern (1991)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Da%20Hong%20D...</td>\n",
       "      <td>['Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boot, Das (1981)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Boot,%20Das%2...</td>\n",
       "      <td>['Action', 'Drama', 'War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?L%2EA%2E+Conf...</td>\n",
       "      <td>['Crime', 'Film-Noir', 'Mystery', 'Thriller']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    movie_title  \\\n",
       "0              Star Wars (1977)   \n",
       "1     African Queen, The (1951)   \n",
       "2  Raise the Red Lantern (1991)   \n",
       "3              Boot, Das (1981)   \n",
       "4      L.A. Confidential (1997)   \n",
       "\n",
       "                                            IMDb_URL  \\\n",
       "0  http://us.imdb.com/M/title-exact?Star%20Wars%2...   \n",
       "1  http://us.imdb.com/M/title-exact?African%20Que...   \n",
       "2  http://us.imdb.com/M/title-exact?Da%20Hong%20D...   \n",
       "3  http://us.imdb.com/M/title-exact?Boot,%20Das%2...   \n",
       "4  http://us.imdb.com/M/title-exact?L%2EA%2E+Conf...   \n",
       "\n",
       "                                         genres_name  \n",
       "0  ['Action', 'Adventure', 'Romance', 'Sci-Fi', '...  \n",
       "1          ['Action', 'Adventure', 'Romance', 'War']  \n",
       "2                                          ['Drama']  \n",
       "3                         ['Action', 'Drama', 'War']  \n",
       "4      ['Crime', 'Film-Noir', 'Mystery', 'Thriller']  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "This is a fairly simple model, and can be extended by adding side-information, adding more non-linear layers, and so on.\n",
    "\n",
    "However, before plunging into such extensions, it is worth knowing that models using explicit ratings have fallen out of favour both in [academia](https://pdfs.semanticscholar.org/8e8e/cc4591f6d919f6ad247e7ef3300de2fed7a3.pdf)  and in [industry](https://media.netflix.com/en/company-blog/goodbye-stars-hello-thumbs). It is now widely accepted that _what_ people choose to interact with is more meaningful than how they rate the interactions they have.\n",
    "\n",
    "These scenarios are called _implicit feedback_ settings. If you're interested in building these models, have a look at Spotlight's [implicit factorization](https://maciejkula.github.io/spotlight/factorization/implicit.html) models, as well as the [implicit sequence models](https://maciejkula.github.io/spotlight/sequence/representations.html) which aim to explicitly model the sequential nature of interaction data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
